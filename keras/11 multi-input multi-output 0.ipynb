{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러개의 텐서를 입력받아 여러개의 텐서를 출력하는 모델을 알아 봅시다. 그 예로서 <br>\n",
    "yA = a * (xA * xB ) + b <br>\n",
    "yB = c * (xA + xB ) + d <br>\n",
    "의 관계를 가지는 데이터에서 a, b, c, d 를 학습으로 발견해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "a = 2; b = 1; c = 2; d = 1\n",
    "\n",
    "x_train_A = np.random.rand(1000,1) * 2 - 1\n",
    "x_train_B = np.random.rand(1000,1) * 2 - 1\n",
    "y_train_A = a * (x_train_A * x_train_B) + b\n",
    "y_train_B = c * (x_train_A + x_train_B) + d\n",
    "\n",
    "xA = layers.Input((1,), name='xA')\n",
    "xB = layers.Input((1,), name='xB')\n",
    "hA = layers.Multiply(name='mul')([xA, xB])\n",
    "hB = layers.Add(name='add')([xA, xB])\n",
    "yA = layers.Dense(1, name='yA')(hA)\n",
    "yB = layers.Dense(1, name='yB')(hB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 정의할때 여러개의 입력과 출력이 있으면 리스트 형식으로 넣어주면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xA (InputLayer)                 (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "xB (InputLayer)                 (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mul (Multiply)                  (None, 1)            0           xA[0][0]                         \n",
      "                                                                 xB[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1)            0           xA[0][0]                         \n",
      "                                                                 xB[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "yA (Dense)                      (None, 1)            2           mul[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "yB (Dense)                      (None, 1)            2           add[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Model([xA, xB], [yA, yB])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 출력에 mse loss 를 적용하고 그것들을 더해 전체 loss 를 정의하려면 다음과 같이 컴파일하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각 출력에 특정 loss 와 가중치를 지정하여 다 더해 전체 loss 를 정의하려면 다음과 같이 컴파일하면 됩니다. 전체 loss 는 하나이므로 그것을 최적화하는 optimizer 는 하나입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss={'yA': 'mse', 'yB': 'mse'},\n",
    "              loss_weights={'yA': 2., 'yB': 1.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 정의할 때처럼 여러 입력과 출력 데이터를 리스트 형식으로 넣고 학습을 합니다. 전체 loss 는 각각 loss 에 가중치를 곱해서 다 더한 값이라는것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 355us/step - loss: 4.4259 - yA_loss: 1.3982 - yB_loss: 1.6296 - val_loss: 3.9702 - val_yA_loss: 1.2271 - val_yB_loss: 1.5160\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 108us/step - loss: 3.7491 - yA_loss: 1.1930 - yB_loss: 1.3632 - val_loss: 3.3518 - val_yA_loss: 1.0421 - val_yB_loss: 1.2677\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 3.1640 - yA_loss: 1.0133 - yB_loss: 1.1375 - val_loss: 2.8128 - val_yA_loss: 0.8814 - val_yB_loss: 1.0500\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 2.6590 - yA_loss: 0.8582 - yB_loss: 0.9427 - val_loss: 2.3558 - val_yA_loss: 0.7447 - val_yB_loss: 0.8663\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 2.2259 - yA_loss: 0.7254 - yB_loss: 0.7750 - val_loss: 1.9621 - val_yA_loss: 0.6272 - val_yB_loss: 0.7077\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 1.8570 - yA_loss: 0.6124 - yB_loss: 0.6323 - val_loss: 1.6325 - val_yA_loss: 0.5290 - val_yB_loss: 0.5746\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 1.5451 - yA_loss: 0.5170 - yB_loss: 0.5112 - val_loss: 1.3481 - val_yA_loss: 0.4437 - val_yB_loss: 0.4606\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 1.2821 - yA_loss: 0.4364 - yB_loss: 0.4093 - val_loss: 1.1147 - val_yA_loss: 0.3743 - val_yB_loss: 0.3661\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.0560 - yA_loss: 0.3668 - yB_loss: 0.322 - 0s 151us/step - loss: 1.0622 - yA_loss: 0.3689 - yB_loss: 0.3245 - val_loss: 0.9216 - val_yA_loss: 0.3165 - val_yB_loss: 0.2886\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 0.8795 - yA_loss: 0.3125 - yB_loss: 0.2546 - val_loss: 0.7612 - val_yA_loss: 0.2681 - val_yB_loss: 0.2249\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.7284 - yA_loss: 0.2654 - yB_loss: 0.1975 - val_loss: 0.6284 - val_yA_loss: 0.2280 - val_yB_loss: 0.1724\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.6043 - yA_loss: 0.2265 - yB_loss: 0.1514 - val_loss: 0.5203 - val_yA_loss: 0.1947 - val_yB_loss: 0.1309\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.5027 - yA_loss: 0.1940 - yB_loss: 0.1147 - val_loss: 0.4315 - val_yA_loss: 0.1668 - val_yB_loss: 0.0979\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.4194 - yA_loss: 0.1669 - yB_loss: 0.0857 - val_loss: 0.3606 - val_yA_loss: 0.1441 - val_yB_loss: 0.0724\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.3514 - yA_loss: 0.1441 - yB_loss: 0.0631 - val_loss: 0.3023 - val_yA_loss: 0.1246 - val_yB_loss: 0.0530\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.2954 - yA_loss: 0.1248 - yB_loss: 0.0458 - val_loss: 0.2555 - val_yA_loss: 0.1086 - val_yB_loss: 0.0382\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.2496 - yA_loss: 0.1084 - yB_loss: 0.0328 - val_loss: 0.2155 - val_yA_loss: 0.0943 - val_yB_loss: 0.0269\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.2115 - yA_loss: 0.0942 - yB_loss: 0.0230 - val_loss: 0.1830 - val_yA_loss: 0.0822 - val_yB_loss: 0.0186\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.1798 - yA_loss: 0.0820 - yB_loss: 0.0159 - val_loss: 0.1555 - val_yA_loss: 0.0714 - val_yB_loss: 0.0126\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 108us/step - loss: 0.1531 - yA_loss: 0.0712 - yB_loss: 0.0107 - val_loss: 0.1329 - val_yA_loss: 0.0622 - val_yB_loss: 0.0085\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.1306 - yA_loss: 0.0617 - yB_loss: 0.0071 - val_loss: 0.1134 - val_yA_loss: 0.0539 - val_yB_loss: 0.0055\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.1113 - yA_loss: 0.0533 - yB_loss: 0.0046 - val_loss: 0.0967 - val_yA_loss: 0.0466 - val_yB_loss: 0.0035\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.0947 - yA_loss: 0.0459 - yB_loss: 0.0029 - val_loss: 0.0823 - val_yA_loss: 0.0400 - val_yB_loss: 0.0022\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0804 - yA_loss: 0.0393 - yB_loss: 0.0018 - val_loss: 0.0697 - val_yA_loss: 0.0342 - val_yB_loss: 0.0013\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.0680 - yA_loss: 0.0335 - yB_loss: 0.0011 - val_loss: 0.0586 - val_yA_loss: 0.0289 - val_yB_loss: 7.9115e-04\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.0572 - yA_loss: 0.0283 - yB_loss: 6.3679e-04 - val_loss: 0.0494 - val_yA_loss: 0.0245 - val_yB_loss: 4.6038e-04\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.0480 - yA_loss: 0.0238 - yB_loss: 3.6272e-04 - val_loss: 0.0413 - val_yA_loss: 0.0205 - val_yB_loss: 2.5284e-04\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.0399 - yA_loss: 0.0199 - yB_loss: 1.9916e-04 - val_loss: 0.0342 - val_yA_loss: 0.0170 - val_yB_loss: 1.3644e-04\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 119us/step - loss: 0.0330 - yA_loss: 0.0164 - yB_loss: 1.0598e-04 - val_loss: 0.0281 - val_yA_loss: 0.0140 - val_yB_loss: 7.1572e-05\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 0.0270 - yA_loss: 0.0135 - yB_loss: 5.4435e-05 - val_loss: 0.0229 - val_yA_loss: 0.0115 - val_yB_loss: 3.6407e-05\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.0219 - yA_loss: 0.0110 - yB_loss: 2.7019e-05 - val_loss: 0.0186 - val_yA_loss: 0.0093 - val_yB_loss: 1.7407e-05\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.0177 - yA_loss: 0.0088 - yB_loss: 1.2847e-05 - val_loss: 0.0149 - val_yA_loss: 0.0074 - val_yB_loss: 8.1084e-06\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.0141 - yA_loss: 0.0070 - yB_loss: 5.8746e-06 - val_loss: 0.0118 - val_yA_loss: 0.0059 - val_yB_loss: 3.6120e-06\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.0111 - yA_loss: 0.0056 - yB_loss: 2.5696e-06 - val_loss: 0.0092 - val_yA_loss: 0.0046 - val_yB_loss: 1.5391e-06\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0087 - yA_loss: 0.0043 - yB_loss: 1.0733e-06 - val_loss: 0.0072 - val_yA_loss: 0.0036 - val_yB_loss: 6.2834e-07\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.0067 - yA_loss: 0.0033 - yB_loss: 4.2938e-07 - val_loss: 0.0055 - val_yA_loss: 0.0027 - val_yB_loss: 2.3930e-07\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.0051 - yA_loss: 0.0025 - yB_loss: 1.6224e-07 - val_loss: 0.0041 - val_yA_loss: 0.0021 - val_yB_loss: 8.7882e-08\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.0038 - yA_loss: 0.0019 - yB_loss: 5.8092e-08 - val_loss: 0.0031 - val_yA_loss: 0.0015 - val_yB_loss: 3.1005e-08\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.0028 - yA_loss: 0.0014 - yB_loss: 1.9735e-08 - val_loss: 0.0023 - val_yA_loss: 0.0011 - val_yB_loss: 1.0206e-08\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.0020 - yA_loss: 0.0010 - yB_loss: 6.3094e-09 - val_loss: 0.0016 - val_yA_loss: 8.1095e-04 - val_yB_loss: 3.1083e-09\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.0015 - yA_loss: 7.3065e-04 - yB_loss: 1.8556e-09 - val_loss: 0.0012 - val_yA_loss: 5.7657e-04 - val_yB_loss: 8.6757e-10\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 102us/step - loss: 0.0010 - yA_loss: 5.1413e-04 - yB_loss: 5.2181e-10 - val_loss: 8.0223e-04 - val_yA_loss: 4.0112e-04 - val_yB_loss: 2.6094e-10\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 124us/step - loss: 7.1016e-04 - yA_loss: 3.5508e-04 - yB_loss: 1.4705e-10 - val_loss: 5.4932e-04 - val_yA_loss: 2.7466e-04 - val_yB_loss: 4.9626e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 100us/step - loss: 4.8163e-04 - yA_loss: 2.4081e-04 - yB_loss: 4.0990e-11 - val_loss: 3.6816e-04 - val_yA_loss: 1.8408e-04 - val_yB_loss: 2.5332e-11\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 103us/step - loss: 3.1940e-04 - yA_loss: 1.5970e-04 - yB_loss: 2.5925e-11 - val_loss: 2.4090e-04 - val_yA_loss: 1.2045e-04 - val_yB_loss: 2.5332e-11\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 2.0727e-04 - yA_loss: 1.0363e-04 - yB_loss: 2.5925e-11 - val_loss: 1.5627e-04 - val_yA_loss: 7.8137e-05 - val_yB_loss: 2.5332e-11\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 1.3186e-04 - yA_loss: 6.5929e-05 - yB_loss: 2.5709e-11 - val_loss: 9.6825e-05 - val_yA_loss: 4.8412e-05 - val_yB_loss: 2.5037e-11\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 8.1827e-05 - yA_loss: 4.0913e-05 - yB_loss: 2.3041e-11 - val_loss: 5.8875e-05 - val_yA_loss: 2.9437e-05 - val_yB_loss: 2.0036e-11\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 98us/step - loss: 4.9331e-05 - yA_loss: 2.4666e-05 - yB_loss: 2.0176e-11 - val_loss: 3.5565e-05 - val_yA_loss: 1.7783e-05 - val_yB_loss: 1.8252e-11\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 102us/step - loss: 2.9075e-05 - yA_loss: 1.4537e-05 - yB_loss: 1.8988e-11 - val_loss: 2.0625e-05 - val_yA_loss: 1.0313e-05 - val_yB_loss: 1.8252e-11\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 103us/step - loss: 1.6636e-05 - yA_loss: 8.3178e-06 - yB_loss: 1.8281e-11 - val_loss: 1.1523e-05 - val_yA_loss: 5.7617e-06 - val_yB_loss: 1.6804e-11\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 9.2256e-06 - yA_loss: 4.6128e-06 - yB_loss: 1.7206e-11 - val_loss: 6.3848e-06 - val_yA_loss: 3.1924e-06 - val_yB_loss: 1.4078e-11\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 4.9802e-06 - yA_loss: 2.4901e-06 - yB_loss: 1.2611e-11 - val_loss: 3.3703e-06 - val_yA_loss: 1.6851e-06 - val_yB_loss: 1.2144e-11\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 2.5934e-06 - yA_loss: 1.2967e-06 - yB_loss: 1.2525e-11 - val_loss: 1.7182e-06 - val_yA_loss: 8.5910e-07 - val_yB_loss: 1.2144e-11\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 1.3058e-06 - yA_loss: 6.5288e-07 - yB_loss: 1.2070e-11 - val_loss: 8.4502e-07 - val_yA_loss: 4.2250e-07 - val_yB_loss: 1.0497e-11\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 6.3314e-07 - yA_loss: 3.1657e-07 - yB_loss: 1.0504e-11 - val_loss: 4.0152e-07 - val_yA_loss: 2.0075e-07 - val_yB_loss: 9.8952e-12\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 2.9652e-07 - yA_loss: 1.4825e-07 - yB_loss: 1.0106e-11 - val_loss: 1.8153e-07 - val_yA_loss: 9.0759e-08 - val_yB_loss: 9.4224e-12\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 109us/step - loss: 1.3310e-07 - yA_loss: 6.6543e-08 - yB_loss: 8.9706e-12 - val_loss: 7.9642e-08 - val_yA_loss: 3.9817e-08 - val_yB_loss: 8.4344e-12\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 114us/step - loss: 5.6974e-08 - yA_loss: 2.8483e-08 - yB_loss: 8.1835e-12 - val_loss: 3.3798e-08 - val_yA_loss: 1.6895e-08 - val_yB_loss: 7.0964e-12\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 104us/step - loss: 2.3505e-08 - yA_loss: 1.1749e-08 - yB_loss: 7.3651e-12 - val_loss: 1.3313e-08 - val_yA_loss: 6.6529e-09 - val_yB_loss: 7.0964e-12\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 99us/step - loss: 9.2157e-09 - yA_loss: 4.6045e-09 - yB_loss: 6.6202e-12 - val_loss: 5.1062e-09 - val_yA_loss: 2.5500e-09 - val_yB_loss: 6.2196e-12\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 102us/step - loss: 3.4358e-09 - yA_loss: 1.7150e-09 - yB_loss: 5.8515e-12 - val_loss: 1.8567e-09 - val_yA_loss: 9.2574e-10 - val_yB_loss: 5.1824e-12\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 97us/step - loss: 1.2220e-09 - yA_loss: 6.0842e-10 - yB_loss: 5.2107e-12 - val_loss: 6.3197e-10 - val_yA_loss: 3.1387e-10 - val_yB_loss: 4.2331e-12\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 101us/step - loss: 3.9629e-10 - yA_loss: 1.9596e-10 - yB_loss: 4.3817e-12 - val_loss: 1.9697e-10 - val_yA_loss: 9.6366e-11 - val_yB_loss: 4.2331e-12\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 1.2966e-10 - yA_loss: 6.2681e-11 - yB_loss: 4.3025e-12 - val_loss: 6.8002e-11 - val_yA_loss: 3.2563e-11 - val_yB_loss: 2.8759e-12\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 3.5683e-11 - yA_loss: 1.6397e-11 - yB_loss: 2.8877e-12 - val_loss: 1.6231e-11 - val_yA_loss: 6.7067e-12 - val_yB_loss: 2.8175e-12\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 1.6271e-11 - yA_loss: 6.6997e-12 - yB_loss: 2.8715e-12 - val_loss: 1.4781e-11 - val_yA_loss: 5.9820e-12 - val_yB_loss: 2.8175e-12\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 1.5240e-11 - yA_loss: 6.1843e-12 - yB_loss: 2.8715e-12 - val_loss: 1.1887e-11 - val_yA_loss: 4.5347e-12 - val_yB_loss: 2.8175e-12\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 1.2481e-11 - yA_loss: 4.8048e-12 - yB_loss: 2.8715e-12 - val_loss: 1.1887e-11 - val_yA_loss: 4.5347e-12 - val_yB_loss: 2.8175e-12\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 1.2297e-11 - yA_loss: 4.7152e-12 - yB_loss: 2.8662e-12 - val_loss: 1.1267e-11 - val_yA_loss: 4.4001e-12 - val_yB_loss: 2.4673e-12\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 103us/step - loss: 1.0087e-11 - yA_loss: 4.2063e-12 - yB_loss: 1.6740e-12 - val_loss: 9.2743e-12 - val_yA_loss: 3.9003e-12 - val_yB_loss: 1.4736e-12\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 9.5756e-12 - yA_loss: 4.0260e-12 - yB_loss: 1.5235e-12 - val_loss: 8.7570e-12 - val_yA_loss: 3.6417e-12 - val_yB_loss: 1.4736e-12\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 8.7196e-12 - yA_loss: 3.5980e-12 - yB_loss: 1.5235e-12 - val_loss: 7.6351e-12 - val_yA_loss: 3.0808e-12 - val_yB_loss: 1.4736e-12\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 7.8285e-12 - yA_loss: 3.1934e-12 - yB_loss: 1.4418e-12 - val_loss: 7.2279e-12 - val_yA_loss: 2.9342e-12 - val_yB_loss: 1.3596e-12\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 6.4909e-12 - yA_loss: 2.5381e-12 - yB_loss: 1.4147e-12 - val_loss: 5.7687e-12 - val_yA_loss: 2.2046e-12 - val_yB_loss: 1.3596e-12\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 6.0990e-12 - yA_loss: 2.3422e-12 - yB_loss: 1.4147e-12 - val_loss: 5.7687e-12 - val_yA_loss: 2.2046e-12 - val_yB_loss: 1.3596e-12\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 5.6144e-12 - yA_loss: 2.1560e-12 - yB_loss: 1.3025e-12 - val_loss: 4.0012e-12 - val_yA_loss: 1.5881e-12 - val_yB_loss: 8.2507e-13\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 108us/step - loss: 4.2183e-12 - yA_loss: 1.6836e-12 - yB_loss: 8.5123e-13 - val_loss: 4.0012e-12 - val_yA_loss: 1.5881e-12 - val_yB_loss: 8.2507e-13\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 108us/step - loss: 4.2183e-12 - yA_loss: 1.6836e-12 - yB_loss: 8.5123e-13 - val_loss: 4.0012e-12 - val_yA_loss: 1.5881e-12 - val_yB_loss: 8.2507e-13\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 4.2183e-12 - yA_loss: 1.6836e-12 - yB_loss: 8.5123e-13 - val_loss: 4.0012e-12 - val_yA_loss: 1.5881e-12 - val_yB_loss: 8.2507e-13\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 3.6113e-12 - yA_loss: 1.5327e-12 - yB_loss: 5.4585e-13 - val_loss: 3.2647e-12 - val_yA_loss: 1.4220e-12 - val_yB_loss: 4.2071e-13\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 3.1944e-12 - yA_loss: 1.3808e-12 - yB_loss: 4.3271e-13 - val_loss: 2.7257e-12 - val_yA_loss: 1.1525e-12 - val_yB_loss: 4.2071e-13\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 110us/step - loss: 2.8553e-12 - yA_loss: 1.2133e-12 - yB_loss: 4.2876e-13 - val_loss: 2.6917e-12 - val_yA_loss: 1.1525e-12 - val_yB_loss: 3.8664e-13\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 100us/step - loss: 2.8238e-12 - yA_loss: 1.2133e-12 - yB_loss: 3.9724e-13 - val_loss: 2.6917e-12 - val_yA_loss: 1.1525e-12 - val_yB_loss: 3.8664e-13\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 101us/step - loss: 2.2553e-12 - yA_loss: 9.2901e-13 - yB_loss: 3.9724e-13 - val_loss: 1.7157e-12 - val_yA_loss: 6.6451e-13 - val_yB_loss: 3.8664e-13\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 100us/step - loss: 1.7681e-12 - yA_loss: 6.8545e-13 - yB_loss: 3.9724e-13 - val_loss: 1.7157e-12 - val_yA_loss: 6.6451e-13 - val_yB_loss: 3.8664e-13\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 1.7608e-12 - yA_loss: 6.8545e-13 - yB_loss: 3.8990e-13 - val_loss: 1.6875e-12 - val_yA_loss: 6.6451e-13 - val_yB_loss: 3.5849e-13\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 1.7477e-12 - yA_loss: 6.8545e-13 - yB_loss: 3.7678e-13 - val_loss: 1.6875e-12 - val_yA_loss: 6.6451e-13 - val_yB_loss: 3.5849e-13\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 1.7298e-12 - yA_loss: 6.8545e-13 - yB_loss: 3.5887e-13 - val_loss: 1.5719e-12 - val_yA_loss: 6.6451e-13 - val_yB_loss: 2.4285e-13\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 1.4361e-12 - yA_loss: 5.9232e-13 - yB_loss: 2.5149e-13 - val_loss: 1.3375e-12 - val_yA_loss: 5.4732e-13 - val_yB_loss: 2.4285e-13\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 1.4004e-12 - yA_loss: 5.7448e-13 - yB_loss: 2.5149e-13 - val_loss: 1.3375e-12 - val_yA_loss: 5.4732e-13 - val_yB_loss: 2.4285e-13\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 108us/step - loss: 1.3445e-12 - yA_loss: 5.4649e-13 - yB_loss: 2.5149e-13 - val_loss: 1.1166e-12 - val_yA_loss: 4.3686e-13 - val_yB_loss: 2.4285e-13\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 102us/step - loss: 1.1727e-12 - yA_loss: 4.6061e-13 - yB_loss: 2.5149e-13 - val_loss: 1.1166e-12 - val_yA_loss: 4.3686e-13 - val_yB_loss: 2.4285e-13\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 102us/step - loss: 1.1727e-12 - yA_loss: 4.6061e-13 - yB_loss: 2.5149e-13 - val_loss: 1.1166e-12 - val_yA_loss: 4.3686e-13 - val_yB_loss: 2.4285e-13\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 103us/step - loss: 8.3924e-13 - yA_loss: 3.6234e-13 - yB_loss: 1.1456e-13 - val_loss: 8.1238e-13 - val_yA_loss: 3.5664e-13 - val_yB_loss: 9.9096e-14\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 99us/step - loss: 7.2756e-13 - yA_loss: 3.1060e-13 - yB_loss: 1.0636e-13 - val_loss: 6.1800e-13 - val_yA_loss: 2.5945e-13 - val_yB_loss: 9.9096e-14\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 103us/step - loss: 6.0596e-13 - yA_loss: 2.4980e-13 - yB_loss: 1.0636e-13 - val_loss: 4.8537e-13 - val_yA_loss: 1.9314e-13 - val_yB_loss: 9.9096e-14\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 5.1565e-13 - yA_loss: 2.0464e-13 - yB_loss: 1.0636e-13 - val_loss: 4.8537e-13 - val_yA_loss: 1.9314e-13 - val_yB_loss: 9.9096e-14\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 5.1565e-13 - yA_loss: 2.0464e-13 - yB_loss: 1.0636e-13 - val_loss: 4.8537e-13 - val_yA_loss: 1.9314e-13 - val_yB_loss: 9.9096e-14\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 98us/step - loss: 5.1565e-13 - yA_loss: 2.0464e-13 - yB_loss: 1.0636e-13 - val_loss: 4.8537e-13 - val_yA_loss: 1.9314e-13 - val_yB_loss: 9.9096e-14\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 103us/step - loss: 5.1565e-13 - yA_loss: 2.0464e-13 - yB_loss: 1.0636e-13 - val_loss: 4.8537e-13 - val_yA_loss: 1.9314e-13 - val_yB_loss: 9.9096e-14\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 5.1235e-13 - yA_loss: 2.0299e-13 - yB_loss: 1.0636e-13 - val_loss: 4.3787e-13 - val_yA_loss: 1.6939e-13 - val_yB_loss: 9.9096e-14\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 103us/step - loss: 4.1402e-13 - yA_loss: 1.7009e-13 - yB_loss: 7.3849e-14 - val_loss: 3.6955e-13 - val_yA_loss: 1.6939e-13 - val_yB_loss: 3.0776e-14\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 3.3369e-13 - yA_loss: 1.5129e-13 - yB_loss: 3.1117e-14 - val_loss: 3.1780e-13 - val_yA_loss: 1.4351e-13 - val_yB_loss: 3.0776e-14\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 3.1737e-13 - yA_loss: 1.4313e-13 - yB_loss: 3.1117e-14 - val_loss: 3.1780e-13 - val_yA_loss: 1.4351e-13 - val_yB_loss: 3.0776e-14\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 100us/step - loss: 3.1737e-13 - yA_loss: 1.4313e-13 - yB_loss: 3.1117e-14 - val_loss: 3.1780e-13 - val_yA_loss: 1.4351e-13 - val_yB_loss: 3.0776e-14\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 2.9855e-13 - yA_loss: 1.3372e-13 - yB_loss: 3.1117e-14 - val_loss: 1.6827e-13 - val_yA_loss: 6.8747e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 1.7607e-13 - yA_loss: 7.2475e-14 - yB_loss: 3.1117e-14 - val_loss: 1.6827e-13 - val_yA_loss: 6.8747e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 106us/step - loss: 1.7607e-13 - yA_loss: 7.2475e-14 - yB_loss: 3.1117e-14 - val_loss: 1.6827e-13 - val_yA_loss: 6.8747e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 108us/step - loss: 1.7607e-13 - yA_loss: 7.2475e-14 - yB_loss: 3.1117e-14 - val_loss: 1.6827e-13 - val_yA_loss: 6.8747e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 1.7607e-13 - yA_loss: 7.2475e-14 - yB_loss: 3.1117e-14 - val_loss: 1.6827e-13 - val_yA_loss: 6.8747e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 1.7607e-13 - yA_loss: 7.2475e-14 - yB_loss: 3.1117e-14 - val_loss: 1.6827e-13 - val_yA_loss: 6.8747e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 1.7607e-13 - yA_loss: 7.2475e-14 - yB_loss: 3.1117e-14 - val_loss: 1.6827e-13 - val_yA_loss: 6.8747e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 106us/step - loss: 1.7607e-13 - yA_loss: 7.2475e-14 - yB_loss: 3.1117e-14 - val_loss: 1.6827e-13 - val_yA_loss: 6.8747e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 1.7607e-13 - yA_loss: 7.2475e-14 - yB_loss: 3.1117e-14 - val_loss: 1.6827e-13 - val_yA_loss: 6.8747e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 1.6135e-13 - yA_loss: 6.5116e-14 - yB_loss: 3.1117e-14 - val_loss: 1.4241e-13 - val_yA_loss: 5.5819e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 108us/step - loss: 1.4637e-13 - yA_loss: 5.7629e-14 - yB_loss: 3.1117e-14 - val_loss: 1.4241e-13 - val_yA_loss: 5.5819e-14 - val_yB_loss: 3.0776e-14\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 1.4208e-13 - yA_loss: 5.7629e-14 - yB_loss: 2.6820e-14 - val_loss: 1.2083e-13 - val_yA_loss: 5.5819e-14 - val_yB_loss: 9.1926e-15\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - ETA: 0s - loss: 8.4738e-14 - yA_loss: 3.7640e-14 - yB_loss: 9.4574e-1 - 0s 105us/step - loss: 6.7028e-14 - yA_loss: 2.9053e-14 - yB_loss: 8.9207e-15 - val_loss: 2.3596e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 9.1926e-15\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 2.6331e-14 - yA_loss: 8.7053e-15 - yB_loss: 8.9207e-15 - val_loss: 2.3596e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 9.1926e-15\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 2.4370e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.9597e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 151us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 103us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 108us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 2.3580e-14 - yA_loss: 8.7053e-15 - yB_loss: 6.1691e-15 - val_loss: 2.1351e-14 - val_yA_loss: 7.2019e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 1.9368e-14 - yA_loss: 6.5992e-15 - yB_loss: 6.1691e-15 - val_loss: 1.7503e-14 - val_yA_loss: 5.2779e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 1.6618e-14 - yA_loss: 5.2245e-15 - yB_loss: 6.1691e-15 - val_loss: 1.7503e-14 - val_yA_loss: 5.2779e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 1.6618e-14 - yA_loss: 5.2245e-15 - yB_loss: 6.1691e-15 - val_loss: 1.7503e-14 - val_yA_loss: 5.2779e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 1.6618e-14 - yA_loss: 5.2245e-15 - yB_loss: 6.1691e-15 - val_loss: 1.7503e-14 - val_yA_loss: 5.2779e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 1.6618e-14 - yA_loss: 5.2245e-15 - yB_loss: 6.1691e-15 - val_loss: 1.7503e-14 - val_yA_loss: 5.2779e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 99us/step - loss: 1.5591e-14 - yA_loss: 4.7107e-15 - yB_loss: 6.1691e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 1.0265e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.1691e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 108us/step - loss: 1.0265e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.1691e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 1.0265e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.1691e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 1.0265e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.1691e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 1.0265e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.1691e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 1.0265e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.1691e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 1.0265e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.1691e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 1.0265e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.1691e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 1.0392e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.2968e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 1.0664e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.5682e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 1.0794e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.6987e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 1.0265e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.1691e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 1.0348e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.2524e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 1.0782e-14 - yA_loss: 2.2410e-15 - yB_loss: 6.2996e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 1.0361e-14 - yA_loss: 2.0478e-15 - yB_loss: 6.2657e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 1.1182e-14 - yA_loss: 2.3032e-15 - yB_loss: 6.5754e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 1.0790e-14 - yA_loss: 2.1422e-15 - yB_loss: 6.5060e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 1.0629e-14 - yA_loss: 2.1911e-15 - yB_loss: 6.2474e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 106us/step - loss: 1.1984e-14 - yA_loss: 2.3631e-15 - yB_loss: 7.2575e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 106us/step - loss: 1.9673e-14 - yA_loss: 2.5427e-15 - yB_loss: 1.4587e-14 - val_loss: 9.3444e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 8.8874e-14\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 2.7743e-14 - yA_loss: 2.4600e-15 - yB_loss: 2.2823e-14 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 100us/step - loss: 1.0943e-14 - yA_loss: 2.2566e-15 - yB_loss: 6.4300e-15 - val_loss: 2.5464e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 2.0894e-14\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 101us/step - loss: 3.7418e-14 - yA_loss: 2.1766e-15 - yB_loss: 3.3064e-14 - val_loss: 1.3762e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 9.1926e-15\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 2.3909e-14 - yA_loss: 2.1478e-15 - yB_loss: 1.9614e-14 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 101us/step - loss: 1.3050e-14 - yA_loss: 2.3232e-15 - yB_loss: 8.4033e-15 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 102us/step - loss: 2.7655e-14 - yA_loss: 2.4597e-15 - yB_loss: 2.2736e-14 - val_loss: 3.5345e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 3.0776e-14\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 106us/step - loss: 1.6352e-14 - yA_loss: 2.3060e-15 - yB_loss: 1.1740e-14 - val_loss: 2.5464e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 2.0894e-14\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 106us/step - loss: 2.8865e-14 - yA_loss: 2.6743e-15 - yB_loss: 2.3517e-14 - val_loss: 3.3105e-14 - val_yA_loss: 6.1056e-15 - val_yB_loss: 2.0894e-14\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 101us/step - loss: 2.2301e-14 - yA_loss: 2.4844e-15 - yB_loss: 1.7332e-14 - val_loss: 1.1516e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 6.9469e-15\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 101us/step - loss: 2.8460e-14 - yA_loss: 4.4304e-15 - yB_loss: 1.9599e-14 - val_loss: 2.9831e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 2.5262e-14\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 2.4700e-13 - yA_loss: 3.4556e-15 - yB_loss: 2.4009e-13 - val_loss: 1.6803e-13 - val_yA_loss: 2.2848e-15 - val_yB_loss: 1.6346e-13\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 2.2232e-13 - yA_loss: 2.4953e-15 - yB_loss: 2.1732e-13 - val_loss: 6.2628e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 5.8058e-14\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 1.5019e-13 - yA_loss: 4.4606e-15 - yB_loss: 1.4127e-13 - val_loss: 5.1592e-14 - val_yA_loss: 6.1056e-15 - val_yB_loss: 3.9380e-14\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 106us/step - loss: 8.3412e-14 - yA_loss: 2.3409e-15 - yB_loss: 7.8730e-14 - val_loss: 1.5962e-13 - val_yA_loss: 6.1056e-15 - val_yB_loss: 1.4741e-13\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 8.4386e-14 - yA_loss: 3.7740e-15 - yB_loss: 7.6838e-14 - val_loss: 5.1592e-14 - val_yA_loss: 6.1056e-15 - val_yB_loss: 3.9380e-14\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 7.5742e-14 - yA_loss: 5.4737e-15 - yB_loss: 6.4794e-14 - val_loss: 4.5112e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 4.0542e-14\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 1.2478e-13 - yA_loss: 3.0215e-15 - yB_loss: 1.1874e-13 - val_loss: 6.2628e-14 - val_yA_loss: 2.2848e-15 - val_yB_loss: 5.8058e-14\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 1.3211e-13 - yA_loss: 9.0542e-15 - yB_loss: 1.1401e-13 - val_loss: 8.1872e-13 - val_yA_loss: 2.3814e-14 - val_yB_loss: 7.7109e-13\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 1.9368e-12 - yA_loss: 1.7066e-14 - yB_loss: 1.9026e-12 - val_loss: 7.8788e-12 - val_yA_loss: 2.2848e-15 - val_yB_loss: 7.8742e-12\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 4.9466e-11 - yA_loss: 2.4749e-14 - yB_loss: 4.9416e-11 - val_loss: 4.3905e-11 - val_yA_loss: 1.6286e-14 - val_yB_loss: 4.3873e-11\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 105us/step - loss: 8.1010e-12 - yA_loss: 1.1702e-14 - yB_loss: 8.0776e-12 - val_loss: 4.6494e-13 - val_yA_loss: 2.2116e-14 - val_yB_loss: 4.2071e-13\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 106us/step - loss: 1.9995e-12 - yA_loss: 3.2243e-14 - yB_loss: 1.9350e-12 - val_loss: 4.3154e-12 - val_yA_loss: 5.5819e-14 - val_yB_loss: 4.2038e-12\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 106us/step - loss: 2.5382e-12 - yA_loss: 6.2798e-14 - yB_loss: 2.4126e-12 - val_loss: 1.0476e-12 - val_yA_loss: 6.1056e-15 - val_yB_loss: 1.0354e-12\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 2.3340e-13 - yA_loss: 2.0324e-14 - yB_loss: 1.9276e-13 - val_loss: 1.1434e-13 - val_yA_loss: 1.6279e-14 - val_yB_loss: 8.1780e-14\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 1.2872e-12 - yA_loss: 1.1370e-14 - yB_loss: 1.2644e-12 - val_loss: 1.0090e-12 - val_yA_loss: 2.2848e-15 - val_yB_loss: 1.0044e-12\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 103us/step - loss: 2.0646e-12 - yA_loss: 3.6737e-14 - yB_loss: 1.9912e-12 - val_loss: 7.1926e-13 - val_yA_loss: 4.0254e-14 - val_yB_loss: 6.3875e-13\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 9.9775e-12 - yA_loss: 1.8322e-14 - yB_loss: 9.9409e-12 - val_loss: 3.6986e-11 - val_yA_loss: 2.2120e-14 - val_yB_loss: 3.6942e-11\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 107us/step - loss: 2.3011e-09 - yA_loss: 1.2334e-14 - yB_loss: 2.3011e-09 - val_loss: 1.8124e-08 - val_yA_loss: 3.7577e-14 - val_yB_loss: 1.8124e-08\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 102us/step - loss: 6.6090e-08 - yA_loss: 3.9204e-14 - yB_loss: 6.6090e-08 - val_loss: 4.5905e-08 - val_yA_loss: 1.9350e-14 - val_yB_loss: 4.5905e-08\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 1.2383e-07 - yA_loss: 7.1119e-14 - yB_loss: 1.2383e-07 - val_loss: 4.1523e-07 - val_yA_loss: 1.1442e-13 - val_yB_loss: 4.1523e-07\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 101us/step - loss: 2.0606e-07 - yA_loss: 7.9743e-14 - yB_loss: 2.0606e-07 - val_loss: 5.0706e-09 - val_yA_loss: 4.4224e-14 - val_yB_loss: 5.0705e-09\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 4.5007e-10 - yA_loss: 1.0265e-13 - yB_loss: 4.4987e-10 - val_loss: 2.7166e-11 - val_yA_loss: 6.2696e-14 - val_yB_loss: 2.7041e-11\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 2.5479e-12 - yA_loss: 1.3621e-13 - yB_loss: 2.2755e-12 - val_loss: 1.3311e-13 - val_yA_loss: 2.2116e-14 - val_yB_loss: 8.8874e-14\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 2.7434e-13 - yA_loss: 7.5602e-14 - yB_loss: 1.2313e-13 - val_loss: 4.3651e-13 - val_yA_loss: 4.0261e-14 - val_yB_loss: 3.5599e-13\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 103us/step - loss: 4.4305e-13 - yA_loss: 1.6114e-13 - yB_loss: 1.2077e-13 - val_loss: 5.3452e-14 - val_yA_loss: 1.6279e-14 - val_yB_loss: 2.0894e-14\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 101us/step - loss: 7.5777e-13 - yA_loss: 3.2388e-13 - yB_loss: 1.1002e-13 - val_loss: 1.1300e-11 - val_yA_loss: 5.6293e-12 - val_yB_loss: 4.1908e-14\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 1.2132e-11 - yA_loss: 6.0380e-12 - yB_loss: 5.6146e-14 - val_loss: 4.2345e-11 - val_yA_loss: 2.1162e-11 - val_yB_loss: 2.0894e-14\n",
      "Epoch 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 142us/step - loss: 9.6754e-11 - yA_loss: 4.8238e-11 - yB_loss: 2.7666e-13 - val_loss: 2.1127e-10 - val_yA_loss: 1.0546e-10 - val_yB_loss: 3.5465e-13\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yA_weights\n",
      "[array([[1.9999828]], dtype=float32), array([0.9999912], dtype=float32)]\n",
      "yB_weights\n",
      "[array([[2.]], dtype=float32), array([1.0000006], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([x_train_A, x_train_B], [y_train_A, y_train_B],\n",
    "                 batch_size=8, epochs=200, validation_split=0.2)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "print('yA_weights'); print(model.get_layer('yA').get_weights())\n",
    "print('yB_weights'); print(model.get_layer('yB').get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
