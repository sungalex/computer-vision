{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fully connected network 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 편미분과 chain rule을 이용하여, forward/backward propagation을 구현한다.\n",
    "- 수학지식을 적용하기 위해서는 적합한 함수를 찾아내는 통찰력이 필요하다.\n",
    "- $s = wx + b$ (식1)\n",
    "\n",
    "\n",
    "- loss function은 MSE를 사용한다.(e로 표현)\n",
    "- $e = L(s,y) = (s-y)^2$ (식2) ---> s는 추정한 값, y는 참 값\n",
    "- w와 b가 변화하면, e가 변화한다. 즉, $w, b \\rightarrow e$ 인 함수관계가 있다.\n",
    "\n",
    "\n",
    "- sample data가 하나라 가정하고, $\\frac{d e}{d w}$와 $\\frac{d e}{d b}$를 구하려면,\n",
    "- (식1)과 (식2)에 chain rule을 적용하여 아래와 같이 계산할 수 있다.<br>\n",
    "  $\\frac{d e}{d w} = \\frac{d s}{d w} \\frac{d e}{d s} = x \\times 2 \\times (s-y)$ <br>\n",
    "  $\\frac{d e}{d b} = \\frac{d s}{d b} \\frac{d e}{d s} = 1 \\times 2 \\times (s-y)$\n",
    "\n",
    "\n",
    "- 샘플이 3개라고 가정하면, gradient descent를 아래와 같이 계산할 수 있다.\n",
    "- $e_{total} = (e_0 + e_1 + e_2) / 3$\n",
    "- $grad\\_w = \\frac{\\delta e_{total}}{\\delta w} = \\left( \\frac{\\delta e_1}{\\delta w} + \\frac{\\delta e_2}{\\delta w} + \\frac{\\delta e_3}{\\delta w} \\right) / 3$\n",
    "- $w \\rightarrow w - grad\\_w \\times lr$ ---> lr: learning rate=0.01\n",
    "- $b \\rightarrow b - grad\\_b \\times lr$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
