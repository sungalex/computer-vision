{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yA = a * xA + b <br>\n",
    "yB = c * exp(yA) + d * xB + e <br>\n",
    "를 만족하는 데이터에서 a, b, c, d, e 를 발견합시다. 초기 웨이트에 따라 학습이 잘 않될 수도 있으니 여러번 실행해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "xA (InputLayer)                 (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "yA (Dense)                      (None, 1)            2           xA[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "exp (Lambda)                    (None, 1)            0           yA[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "xB (InputLayer)                 (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 2)            0           exp[0][0]                        \n",
      "                                                                 xB[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "yB (Dense)                      (None, 1)            3           concat[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 1s 668us/step - loss: 234.5409 - yA_loss: 2.2162 - yB_loss: 232.3246 - val_loss: 269.1432 - val_yA_loss: 2.1909 - val_yB_loss: 266.9523\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 229.3072 - yA_loss: 1.7962 - yB_loss: 227.5109 - val_loss: 262.4273 - val_yA_loss: 1.6881 - val_yB_loss: 260.7392\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 222.2437 - yA_loss: 1.3463 - yB_loss: 220.8974 - val_loss: 253.1989 - val_yA_loss: 1.2354 - val_yB_loss: 251.9635\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 212.0829 - yA_loss: 0.9463 - yB_loss: 211.1366 - val_loss: 239.5282 - val_yA_loss: 0.8443 - val_yB_loss: 238.6839\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 196.8377 - yA_loss: 0.6169 - yB_loss: 196.2208 - val_loss: 218.2962 - val_yA_loss: 0.5326 - val_yB_loss: 217.7636\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 174.1423 - yA_loss: 0.3807 - yB_loss: 173.7616 - val_loss: 187.4308 - val_yA_loss: 0.3282 - val_yB_loss: 187.1026\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 142.2585 - yA_loss: 0.2461 - yB_loss: 142.0124 - val_loss: 145.0943 - val_yA_loss: 0.2285 - val_yB_loss: 144.8658\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 100.4999 - yA_loss: 0.2069 - yB_loss: 100.2930 - val_loss: 93.4621 - val_yA_loss: 0.2261 - val_yB_loss: 93.2360\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 56.2532 - yA_loss: 0.2559 - yB_loss: 55.9973 - val_loss: 42.7177 - val_yA_loss: 0.3019 - val_yB_loss: 42.4158\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 22.3253 - yA_loss: 0.3504 - yB_loss: 21.9749 - val_loss: 14.4458 - val_yA_loss: 0.3960 - val_yB_loss: 14.0498\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 7.8349 - yA_loss: 0.4337 - yB_loss: 7.4012 - val_loss: 5.5771 - val_yA_loss: 0.4588 - val_yB_loss: 5.1183\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 4.8068 - yA_loss: 0.4728 - yB_loss: 4.3340 - val_loss: 4.2034 - val_yA_loss: 0.4772 - val_yB_loss: 3.7262\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 4.4166 - yA_loss: 0.4806 - yB_loss: 3.9360 - val_loss: 3.8942 - val_yA_loss: 0.4776 - val_yB_loss: 3.4166\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 4.2431 - yA_loss: 0.4768 - yB_loss: 3.7663 - val_loss: 3.7285 - val_yA_loss: 0.4727 - val_yB_loss: 3.2558\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 4.0888 - yA_loss: 0.4726 - yB_loss: 3.6162 - val_loss: 3.5964 - val_yA_loss: 0.4658 - val_yB_loss: 3.1306\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 3.9375 - yA_loss: 0.4640 - yB_loss: 3.4735 - val_loss: 3.4450 - val_yA_loss: 0.4601 - val_yB_loss: 2.9849\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 3.7894 - yA_loss: 0.4588 - yB_loss: 3.3306 - val_loss: 3.3600 - val_yA_loss: 0.4506 - val_yB_loss: 2.9094\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 3.6378 - yA_loss: 0.4513 - yB_loss: 3.1866 - val_loss: 3.1982 - val_yA_loss: 0.4461 - val_yB_loss: 2.7521\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 3.4873 - yA_loss: 0.4452 - yB_loss: 3.0421 - val_loss: 3.0516 - val_yA_loss: 0.4411 - val_yB_loss: 2.6105\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 3.3468 - yA_loss: 0.4395 - yB_loss: 2.9073 - val_loss: 2.9454 - val_yA_loss: 0.4334 - val_yB_loss: 2.5120\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 3.2042 - yA_loss: 0.4332 - yB_loss: 2.7710 - val_loss: 2.8064 - val_yA_loss: 0.4280 - val_yB_loss: 2.3785\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 3.0683 - yA_loss: 0.4270 - yB_loss: 2.6413 - val_loss: 2.6947 - val_yA_loss: 0.4217 - val_yB_loss: 2.2731\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 2.9328 - yA_loss: 0.4188 - yB_loss: 2.5141 - val_loss: 2.5547 - val_yA_loss: 0.4180 - val_yB_loss: 2.1368\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 2.8061 - yA_loss: 0.4150 - yB_loss: 2.3911 - val_loss: 2.4352 - val_yA_loss: 0.4125 - val_yB_loss: 2.0227\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 2.6779 - yA_loss: 0.4098 - yB_loss: 2.2680 - val_loss: 2.3411 - val_yA_loss: 0.4055 - val_yB_loss: 1.9356\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 2.5549 - yA_loss: 0.4035 - yB_loss: 2.1514 - val_loss: 2.2305 - val_yA_loss: 0.4005 - val_yB_loss: 1.8300\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 2.4409 - yA_loss: 0.3980 - yB_loss: 2.0429 - val_loss: 2.1151 - val_yA_loss: 0.3969 - val_yB_loss: 1.7182\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 2.3239 - yA_loss: 0.3931 - yB_loss: 1.9309 - val_loss: 2.0445 - val_yA_loss: 0.3887 - val_yB_loss: 1.6558\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 2.2124 - yA_loss: 0.3878 - yB_loss: 1.8246 - val_loss: 1.9498 - val_yA_loss: 0.3835 - val_yB_loss: 1.5663\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 155us/step - loss: 2.1063 - yA_loss: 0.3819 - yB_loss: 1.7244 - val_loss: 1.8623 - val_yA_loss: 0.3781 - val_yB_loss: 1.4842\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 2.0022 - yA_loss: 0.3779 - yB_loss: 1.6242 - val_loss: 1.7655 - val_yA_loss: 0.3741 - val_yB_loss: 1.3914\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 1.9065 - yA_loss: 0.3719 - yB_loss: 1.5346 - val_loss: 1.6912 - val_yA_loss: 0.3683 - val_yB_loss: 1.3229\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 1.8078 - yA_loss: 0.3685 - yB_loss: 1.4394 - val_loss: 1.6041 - val_yA_loss: 0.3645 - val_yB_loss: 1.2396\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 1.7196 - yA_loss: 0.3632 - yB_loss: 1.3564 - val_loss: 1.5032 - val_yA_loss: 0.3624 - val_yB_loss: 1.1408\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 1.6287 - yA_loss: 0.3599 - yB_loss: 1.2688 - val_loss: 1.4522 - val_yA_loss: 0.3554 - val_yB_loss: 1.0968\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 1.5434 - yA_loss: 0.3539 - yB_loss: 1.1895 - val_loss: 1.3548 - val_yA_loss: 0.3540 - val_yB_loss: 1.0008\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 1.4638 - yA_loss: 0.3509 - yB_loss: 1.1129 - val_loss: 1.3065 - val_yA_loss: 0.3470 - val_yB_loss: 0.9595\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 1.3848 - yA_loss: 0.3467 - yB_loss: 1.0380 - val_loss: 1.2544 - val_yA_loss: 0.3415 - val_yB_loss: 0.9129\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 1.3085 - yA_loss: 0.3415 - yB_loss: 0.9670 - val_loss: 1.1573 - val_yA_loss: 0.3410 - val_yB_loss: 0.8163\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 1.2420 - yA_loss: 0.3387 - yB_loss: 0.9033 - val_loss: 1.1007 - val_yA_loss: 0.3361 - val_yB_loss: 0.7646\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 1.1699 - yA_loss: 0.3340 - yB_loss: 0.8359 - val_loss: 1.0373 - val_yA_loss: 0.3327 - val_yB_loss: 0.7046\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 1.1039 - yA_loss: 0.3302 - yB_loss: 0.7736 - val_loss: 0.9865 - val_yA_loss: 0.3276 - val_yB_loss: 0.6589\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 1.0409 - yA_loss: 0.3259 - yB_loss: 0.7150 - val_loss: 0.9335 - val_yA_loss: 0.3232 - val_yB_loss: 0.6103\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.9811 - yA_loss: 0.3217 - yB_loss: 0.6593 - val_loss: 0.8805 - val_yA_loss: 0.3195 - val_yB_loss: 0.5610\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 0.9256 - yA_loss: 0.3183 - yB_loss: 0.6073 - val_loss: 0.8412 - val_yA_loss: 0.3138 - val_yB_loss: 0.5274\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.8715 - yA_loss: 0.3126 - yB_loss: 0.5588 - val_loss: 0.7819 - val_yA_loss: 0.3114 - val_yB_loss: 0.4705\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.8162 - yA_loss: 0.3088 - yB_loss: 0.5074 - val_loss: 0.7344 - val_yA_loss: 0.3106 - val_yB_loss: 0.4238\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.7735 - yA_loss: 0.3057 - yB_loss: 0.4678 - val_loss: 0.6948 - val_yA_loss: 0.3031 - val_yB_loss: 0.3916\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.7242 - yA_loss: 0.3011 - yB_loss: 0.4231 - val_loss: 0.6688 - val_yA_loss: 0.2967 - val_yB_loss: 0.3722\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.6788 - yA_loss: 0.2963 - yB_loss: 0.3826 - val_loss: 0.6177 - val_yA_loss: 0.2942 - val_yB_loss: 0.3235\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.6372 - yA_loss: 0.2925 - yB_loss: 0.3447 - val_loss: 0.5942 - val_yA_loss: 0.2879 - val_yB_loss: 0.3063\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.5988 - yA_loss: 0.2873 - yB_loss: 0.3115 - val_loss: 0.5470 - val_yA_loss: 0.2857 - val_yB_loss: 0.2613\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.5632 - yA_loss: 0.2837 - yB_loss: 0.2795 - val_loss: 0.5394 - val_yA_loss: 0.2777 - val_yB_loss: 0.2617\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.5303 - yA_loss: 0.2778 - yB_loss: 0.2525 - val_loss: 0.4900 - val_yA_loss: 0.2751 - val_yB_loss: 0.2149\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.4956 - yA_loss: 0.2739 - yB_loss: 0.2217 - val_loss: 0.4724 - val_yA_loss: 0.2690 - val_yB_loss: 0.2033\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.4655 - yA_loss: 0.2685 - yB_loss: 0.1970 - val_loss: 0.4309 - val_yA_loss: 0.2684 - val_yB_loss: 0.1625\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.4386 - yA_loss: 0.2643 - yB_loss: 0.1742 - val_loss: 0.4077 - val_yA_loss: 0.2614 - val_yB_loss: 0.1463\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.4120 - yA_loss: 0.2588 - yB_loss: 0.1532 - val_loss: 0.3928 - val_yA_loss: 0.2548 - val_yB_loss: 0.1381\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.3885 - yA_loss: 0.2536 - yB_loss: 0.1349 - val_loss: 0.3621 - val_yA_loss: 0.2523 - val_yB_loss: 0.1098\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.3661 - yA_loss: 0.2484 - yB_loss: 0.1177 - val_loss: 0.3426 - val_yA_loss: 0.2461 - val_yB_loss: 0.0966\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.3437 - yA_loss: 0.2433 - yB_loss: 0.1005 - val_loss: 0.3297 - val_yA_loss: 0.2393 - val_yB_loss: 0.0905\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.3251 - yA_loss: 0.2376 - yB_loss: 0.0875 - val_loss: 0.3069 - val_yA_loss: 0.2353 - val_yB_loss: 0.0716\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.3066 - yA_loss: 0.2322 - yB_loss: 0.0744 - val_loss: 0.2932 - val_yA_loss: 0.2287 - val_yB_loss: 0.0645\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.2901 - yA_loss: 0.2264 - yB_loss: 0.0637 - val_loss: 0.2799 - val_yA_loss: 0.2226 - val_yB_loss: 0.0573\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.2742 - yA_loss: 0.2208 - yB_loss: 0.0535 - val_loss: 0.2620 - val_yA_loss: 0.2183 - val_yB_loss: 0.0437\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.2596 - yA_loss: 0.2151 - yB_loss: 0.0446 - val_loss: 0.2492 - val_yA_loss: 0.2122 - val_yB_loss: 0.0370\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.2470 - yA_loss: 0.2092 - yB_loss: 0.0377 - val_loss: 0.2384 - val_yA_loss: 0.2056 - val_yB_loss: 0.0329\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.2340 - yA_loss: 0.2031 - yB_loss: 0.0309 - val_loss: 0.2260 - val_yA_loss: 0.2001 - val_yB_loss: 0.0259\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.2227 - yA_loss: 0.1973 - yB_loss: 0.0254 - val_loss: 0.2151 - val_yA_loss: 0.1943 - val_yB_loss: 0.0208\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.2118 - yA_loss: 0.1911 - yB_loss: 0.0207 - val_loss: 0.2059 - val_yA_loss: 0.1875 - val_yB_loss: 0.0184\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.2016 - yA_loss: 0.1849 - yB_loss: 0.0167 - val_loss: 0.1959 - val_yA_loss: 0.1824 - val_yB_loss: 0.0134\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.1923 - yA_loss: 0.1788 - yB_loss: 0.0135 - val_loss: 0.1869 - val_yA_loss: 0.1756 - val_yB_loss: 0.0114\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.1832 - yA_loss: 0.1726 - yB_loss: 0.0106 - val_loss: 0.1788 - val_yA_loss: 0.1691 - val_yB_loss: 0.0096\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.1748 - yA_loss: 0.1663 - yB_loss: 0.0085 - val_loss: 0.1707 - val_yA_loss: 0.1629 - val_yB_loss: 0.0078\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.1668 - yA_loss: 0.1600 - yB_loss: 0.0068 - val_loss: 0.1627 - val_yA_loss: 0.1568 - val_yB_loss: 0.0059\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.1592 - yA_loss: 0.1537 - yB_loss: 0.0055 - val_loss: 0.1553 - val_yA_loss: 0.1507 - val_yB_loss: 0.0046\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.1519 - yA_loss: 0.1477 - yB_loss: 0.0043 - val_loss: 0.1483 - val_yA_loss: 0.1449 - val_yB_loss: 0.0034\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.1448 - yA_loss: 0.1414 - yB_loss: 0.0034 - val_loss: 0.1415 - val_yA_loss: 0.1382 - val_yB_loss: 0.0033\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 132us/step - loss: 0.1381 - yA_loss: 0.1352 - yB_loss: 0.0029 - val_loss: 0.1349 - val_yA_loss: 0.1321 - val_yB_loss: 0.0027\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.1315 - yA_loss: 0.1292 - yB_loss: 0.0024 - val_loss: 0.1285 - val_yA_loss: 0.1261 - val_yB_loss: 0.0025\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.1253 - yA_loss: 0.1230 - yB_loss: 0.0022 - val_loss: 0.1223 - val_yA_loss: 0.1203 - val_yB_loss: 0.0020\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 0.1193 - yA_loss: 0.1172 - yB_loss: 0.0021 - val_loss: 0.1166 - val_yA_loss: 0.1140 - val_yB_loss: 0.0025\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.1133 - yA_loss: 0.1112 - yB_loss: 0.0021 - val_loss: 0.1106 - val_yA_loss: 0.1085 - val_yB_loss: 0.0021\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 0.1076 - yA_loss: 0.1056 - yB_loss: 0.0020 - val_loss: 0.1052 - val_yA_loss: 0.1026 - val_yB_loss: 0.0025\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.1021 - yA_loss: 0.0998 - yB_loss: 0.0023 - val_loss: 0.0996 - val_yA_loss: 0.0974 - val_yB_loss: 0.0022\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.0968 - yA_loss: 0.0945 - yB_loss: 0.0023 - val_loss: 0.0946 - val_yA_loss: 0.0917 - val_yB_loss: 0.0029\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.0916 - yA_loss: 0.0892 - yB_loss: 0.0024 - val_loss: 0.0895 - val_yA_loss: 0.0868 - val_yB_loss: 0.0027\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.0866 - yA_loss: 0.0838 - yB_loss: 0.0027 - val_loss: 0.0847 - val_yA_loss: 0.0818 - val_yB_loss: 0.0029\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.0818 - yA_loss: 0.0789 - yB_loss: 0.0029 - val_loss: 0.0798 - val_yA_loss: 0.0767 - val_yB_loss: 0.0031\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.0772 - yA_loss: 0.0741 - yB_loss: 0.0031 - val_loss: 0.0753 - val_yA_loss: 0.0718 - val_yB_loss: 0.0034\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.0727 - yA_loss: 0.0695 - yB_loss: 0.0032 - val_loss: 0.0710 - val_yA_loss: 0.0672 - val_yB_loss: 0.0037\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 0.0684 - yA_loss: 0.0650 - yB_loss: 0.0034 - val_loss: 0.0667 - val_yA_loss: 0.0631 - val_yB_loss: 0.0037\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.0645 - yA_loss: 0.0608 - yB_loss: 0.0037 - val_loss: 0.0628 - val_yA_loss: 0.0590 - val_yB_loss: 0.0038\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.0604 - yA_loss: 0.0566 - yB_loss: 0.0039 - val_loss: 0.0596 - val_yA_loss: 0.0553 - val_yB_loss: 0.0043\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.0569 - yA_loss: 0.0528 - yB_loss: 0.0041 - val_loss: 0.0552 - val_yA_loss: 0.0512 - val_yB_loss: 0.0040\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.0531 - yA_loss: 0.0491 - yB_loss: 0.0040 - val_loss: 0.0526 - val_yA_loss: 0.0479 - val_yB_loss: 0.0047\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 0.0498 - yA_loss: 0.0456 - yB_loss: 0.0042 - val_loss: 0.0483 - val_yA_loss: 0.0440 - val_yB_loss: 0.0043\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.0463 - yA_loss: 0.0423 - yB_loss: 0.0040 - val_loss: 0.0454 - val_yA_loss: 0.0412 - val_yB_loss: 0.0042\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.0431 - yA_loss: 0.0392 - yB_loss: 0.0039 - val_loss: 0.0435 - val_yA_loss: 0.0383 - val_yB_loss: 0.0052\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.0402 - yA_loss: 0.0363 - yB_loss: 0.0039 - val_loss: 0.0391 - val_yA_loss: 0.0350 - val_yB_loss: 0.0041\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0373 - yA_loss: 0.0334 - yB_loss: 0.0039 - val_loss: 0.0364 - val_yA_loss: 0.0324 - val_yB_loss: 0.0040\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.0348 - yA_loss: 0.0308 - yB_loss: 0.0040 - val_loss: 0.0341 - val_yA_loss: 0.0295 - val_yB_loss: 0.0046\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 0.0321 - yA_loss: 0.0284 - yB_loss: 0.0037 - val_loss: 0.0314 - val_yA_loss: 0.0276 - val_yB_loss: 0.0038\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0298 - yA_loss: 0.0262 - yB_loss: 0.0037 - val_loss: 0.0288 - val_yA_loss: 0.0251 - val_yB_loss: 0.0037\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.0274 - yA_loss: 0.0240 - yB_loss: 0.0034 - val_loss: 0.0265 - val_yA_loss: 0.0232 - val_yB_loss: 0.0033\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.0257 - yA_loss: 0.0221 - yB_loss: 0.0036 - val_loss: 0.0244 - val_yA_loss: 0.0212 - val_yB_loss: 0.0032\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.0233 - yA_loss: 0.0202 - yB_loss: 0.0031 - val_loss: 0.0225 - val_yA_loss: 0.0195 - val_yB_loss: 0.0030\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.0213 - yA_loss: 0.0184 - yB_loss: 0.0029 - val_loss: 0.0210 - val_yA_loss: 0.0180 - val_yB_loss: 0.0030\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 0.0197 - yA_loss: 0.0169 - yB_loss: 0.0028 - val_loss: 0.0188 - val_yA_loss: 0.0162 - val_yB_loss: 0.0026\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.0179 - yA_loss: 0.0154 - yB_loss: 0.0025 - val_loss: 0.0178 - val_yA_loss: 0.0145 - val_yB_loss: 0.0033\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.0162 - yA_loss: 0.0140 - yB_loss: 0.0023 - val_loss: 0.0158 - val_yA_loss: 0.0135 - val_yB_loss: 0.0023\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.0149 - yA_loss: 0.0127 - yB_loss: 0.0022 - val_loss: 0.0154 - val_yA_loss: 0.0119 - val_yB_loss: 0.0035\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.0134 - yA_loss: 0.0115 - yB_loss: 0.0019 - val_loss: 0.0130 - val_yA_loss: 0.0110 - val_yB_loss: 0.0019\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.0122 - yA_loss: 0.0104 - yB_loss: 0.0018 - val_loss: 0.0123 - val_yA_loss: 0.0101 - val_yB_loss: 0.0022\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.0111 - yA_loss: 0.0094 - yB_loss: 0.0017 - val_loss: 0.0109 - val_yA_loss: 0.0088 - val_yB_loss: 0.0021\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.0098 - yA_loss: 0.0084 - yB_loss: 0.0014 - val_loss: 0.0095 - val_yA_loss: 0.0080 - val_yB_loss: 0.0016\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.0088 - yA_loss: 0.0076 - yB_loss: 0.0012 - val_loss: 0.0084 - val_yA_loss: 0.0072 - val_yB_loss: 0.0012\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 0.0079 - yA_loss: 0.0068 - yB_loss: 0.0011 - val_loss: 0.0078 - val_yA_loss: 0.0063 - val_yB_loss: 0.0015\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.0070 - yA_loss: 0.0060 - yB_loss: 0.0010 - val_loss: 0.0067 - val_yA_loss: 0.0057 - val_yB_loss: 0.0010\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.0062 - yA_loss: 0.0053 - yB_loss: 8.9326e-04 - val_loss: 0.0060 - val_yA_loss: 0.0050 - val_yB_loss: 9.9905e-04\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 0.0055 - yA_loss: 0.0047 - yB_loss: 8.4267e-04 - val_loss: 0.0052 - val_yA_loss: 0.0044 - val_yB_loss: 7.7319e-04\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0048 - yA_loss: 0.0041 - yB_loss: 7.0069e-04 - val_loss: 0.0046 - val_yA_loss: 0.0039 - val_yB_loss: 7.1002e-04\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.0043 - yA_loss: 0.0036 - yB_loss: 6.5700e-04 - val_loss: 0.0040 - val_yA_loss: 0.0034 - val_yB_loss: 5.8889e-04\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 0.0037 - yA_loss: 0.0031 - yB_loss: 5.5114e-04 - val_loss: 0.0035 - val_yA_loss: 0.0030 - val_yB_loss: 4.8373e-04\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 0.0032 - yA_loss: 0.0027 - yB_loss: 4.5485e-04 - val_loss: 0.0031 - val_yA_loss: 0.0026 - val_yB_loss: 5.3067e-04\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.0029 - yA_loss: 0.0024 - yB_loss: 5.0039e-04 - val_loss: 0.0028 - val_yA_loss: 0.0021 - val_yB_loss: 6.8724e-04\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.0024 - yA_loss: 0.0020 - yB_loss: 3.7768e-04 - val_loss: 0.0022 - val_yA_loss: 0.0019 - val_yB_loss: 3.7393e-04\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 0.0021 - yA_loss: 0.0017 - yB_loss: 3.3895e-04 - val_loss: 0.0019 - val_yA_loss: 0.0016 - val_yB_loss: 2.6482e-04\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.0017 - yA_loss: 0.0015 - yB_loss: 2.5683e-04 - val_loss: 0.0017 - val_yA_loss: 0.0014 - val_yB_loss: 3.2926e-04\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 0.0015 - yA_loss: 0.0012 - yB_loss: 2.6003e-04 - val_loss: 0.0013 - val_yA_loss: 0.0011 - val_yB_loss: 1.8588e-04\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.0012 - yA_loss: 0.0010 - yB_loss: 1.9736e-04 - val_loss: 0.0012 - val_yA_loss: 9.7764e-04 - val_yB_loss: 2.6498e-04\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.0011 - yA_loss: 8.6745e-04 - yB_loss: 1.9173e-04 - val_loss: 9.2191e-04 - val_yA_loss: 7.8642e-04 - val_yB_loss: 1.3549e-04\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 8.3666e-04 - yA_loss: 7.1490e-04 - yB_loss: 1.2177e-04 - val_loss: 7.6559e-04 - val_yA_loss: 6.4462e-04 - val_yB_loss: 1.2098e-04\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 7.0858e-04 - yA_loss: 5.8541e-04 - yB_loss: 1.2316e-04 - val_loss: 7.8839e-04 - val_yA_loss: 5.5013e-04 - val_yB_loss: 2.3826e-04\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 6.1305e-04 - yA_loss: 4.7965e-04 - yB_loss: 1.3340e-04 - val_loss: 7.4734e-04 - val_yA_loss: 4.5129e-04 - val_yB_loss: 2.9605e-04\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 4.7872e-04 - yA_loss: 3.8494e-04 - yB_loss: 9.3784e-05 - val_loss: 4.1956e-04 - val_yA_loss: 3.5017e-04 - val_yB_loss: 6.9391e-05\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 3.6407e-04 - yA_loss: 3.0796e-04 - yB_loss: 5.6107e-05 - val_loss: 3.2539e-04 - val_yA_loss: 2.7831e-04 - val_yB_loss: 4.7086e-05\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 2.9136e-04 - yA_loss: 2.4606e-04 - yB_loss: 4.5298e-05 - val_loss: 2.6516e-04 - val_yA_loss: 2.1997e-04 - val_yB_loss: 4.5192e-05\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 2.3820e-04 - yA_loss: 1.9253e-04 - yB_loss: 4.5665e-05 - val_loss: 2.8711e-04 - val_yA_loss: 1.7857e-04 - val_yB_loss: 1.0854e-04\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 1.8455e-04 - yA_loss: 1.4991e-04 - yB_loss: 3.4633e-05 - val_loss: 1.6914e-04 - val_yA_loss: 1.3498e-04 - val_yB_loss: 3.4160e-05\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 1.4111e-04 - yA_loss: 1.1539e-04 - yB_loss: 2.5721e-05 - val_loss: 1.5487e-04 - val_yA_loss: 9.6280e-05 - val_yB_loss: 5.8590e-05\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 1.0707e-04 - yA_loss: 8.7418e-05 - yB_loss: 1.9656e-05 - val_loss: 9.1760e-05 - val_yA_loss: 7.7969e-05 - val_yB_loss: 1.3791e-05\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 186us/step - loss: 7.7247e-05 - yA_loss: 6.5636e-05 - yB_loss: 1.1611e-05 - val_loss: 6.6873e-05 - val_yA_loss: 5.7051e-05 - val_yB_loss: 9.8219e-06\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 5.8228e-05 - yA_loss: 4.8543e-05 - yB_loss: 9.6843e-06 - val_loss: 8.3950e-05 - val_yA_loss: 4.3836e-05 - val_yB_loss: 4.0114e-05\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 6.2606e-05 - yA_loss: 3.4892e-05 - yB_loss: 2.7714e-05 - val_loss: 3.3281e-04 - val_yA_loss: 2.5041e-05 - val_yB_loss: 3.0777e-04\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 9.5315e-05 - yA_loss: 2.5825e-05 - yB_loss: 6.9491e-05 - val_loss: 2.6948e-05 - val_yA_loss: 2.1441e-05 - val_yB_loss: 5.5064e-06\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 2.1804e-05 - yA_loss: 1.8274e-05 - yB_loss: 3.5304e-06 - val_loss: 1.8638e-05 - val_yA_loss: 1.5528e-05 - val_yB_loss: 3.1094e-06\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 1.5264e-05 - yA_loss: 1.2650e-05 - yB_loss: 2.6139e-06 - val_loss: 1.2199e-05 - val_yA_loss: 1.0497e-05 - val_yB_loss: 1.7021e-06\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 1.0560e-05 - yA_loss: 8.7793e-06 - yB_loss: 1.7806e-06 - val_loss: 9.6253e-06 - val_yA_loss: 7.4456e-06 - val_yB_loss: 2.1797e-06\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 111us/step - loss: 8.2139e-06 - yA_loss: 5.8838e-06 - yB_loss: 2.3300e-06 - val_loss: 1.0712e-05 - val_yA_loss: 5.0870e-06 - val_yB_loss: 5.6254e-06\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 8.6098e-06 - yA_loss: 3.9134e-06 - yB_loss: 4.6964e-06 - val_loss: 3.6593e-06 - val_yA_loss: 3.1495e-06 - val_yB_loss: 5.0979e-07\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 3.1423e-06 - yA_loss: 2.5361e-06 - yB_loss: 6.0618e-07 - val_loss: 2.3864e-06 - val_yA_loss: 1.9648e-06 - val_yB_loss: 4.2159e-07\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 2.3217e-06 - yA_loss: 1.6053e-06 - yB_loss: 7.1637e-07 - val_loss: 1.4745e-06 - val_yA_loss: 1.2639e-06 - val_yB_loss: 2.1059e-07\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 1.2435e-06 - yA_loss: 1.0079e-06 - yB_loss: 2.3560e-07 - val_loss: 1.3393e-06 - val_yA_loss: 8.1426e-07 - val_yB_loss: 5.2501e-07\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 8.1339e-07 - yA_loss: 6.0626e-07 - yB_loss: 2.0714e-07 - val_loss: 2.5872e-06 - val_yA_loss: 5.3071e-07 - val_yB_loss: 2.0565e-06\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 6.4281e-07 - yA_loss: 3.5604e-07 - yB_loss: 2.8677e-07 - val_loss: 3.6190e-07 - val_yA_loss: 2.7048e-07 - val_yB_loss: 9.1414e-08\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 4.8446e-07 - yA_loss: 2.0296e-07 - yB_loss: 2.8150e-07 - val_loss: 2.3569e-07 - val_yA_loss: 1.4257e-07 - val_yB_loss: 9.3121e-08\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 1.5713e-07 - yA_loss: 1.1455e-07 - yB_loss: 4.2572e-08 - val_loss: 1.4231e-07 - val_yA_loss: 7.9996e-08 - val_yB_loss: 6.2312e-08\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 2.0426e-07 - yA_loss: 6.1165e-08 - yB_loss: 1.4310e-07 - val_loss: 3.0612e-07 - val_yA_loss: 3.6738e-08 - val_yB_loss: 2.6938e-07\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 7.6271e-08 - yA_loss: 3.2688e-08 - yB_loss: 4.3583e-08 - val_loss: 5.8569e-08 - val_yA_loss: 2.0866e-08 - val_yB_loss: 3.7703e-08\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 4.6273e-07 - yA_loss: 1.6784e-08 - yB_loss: 4.4594e-07 - val_loss: 2.4821e-08 - val_yA_loss: 1.1295e-08 - val_yB_loss: 1.3526e-08\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 1.9404e-08 - yA_loss: 7.9430e-09 - yB_loss: 1.1461e-08 - val_loss: 7.3378e-09 - val_yA_loss: 4.9893e-09 - val_yB_loss: 2.3486e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 8.9642e-08 - yA_loss: 4.0173e-09 - yB_loss: 8.5624e-08 - val_loss: 6.5194e-07 - val_yA_loss: 5.1705e-09 - val_yB_loss: 6.4677e-07\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 1.1915e-06 - yA_loss: 3.6819e-09 - yB_loss: 1.1879e-06 - val_loss: 2.1003e-08 - val_yA_loss: 1.4325e-09 - val_yB_loss: 1.9571e-08\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 118us/step - loss: 1.8155e-07 - yA_loss: 1.2299e-09 - yB_loss: 1.8032e-07 - val_loss: 4.2145e-09 - val_yA_loss: 5.7247e-10 - val_yB_loss: 3.6420e-09\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 7.3888e-08 - yA_loss: 4.1498e-10 - yB_loss: 7.3473e-08 - val_loss: 1.7124e-07 - val_yA_loss: 3.9748e-10 - val_yB_loss: 1.7085e-07\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 2.5892e-08 - yA_loss: 2.3257e-10 - yB_loss: 2.5659e-08 - val_loss: 6.9373e-09 - val_yA_loss: 1.1734e-10 - val_yB_loss: 6.8199e-09\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 2.6647e-09 - yA_loss: 4.3973e-11 - yB_loss: 2.6207e-09 - val_loss: 9.8907e-10 - val_yA_loss: 4.5126e-11 - val_yB_loss: 9.4394e-10\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 3.9485e-10 - yA_loss: 2.1125e-11 - yB_loss: 3.7372e-10 - val_loss: 7.9399e-09 - val_yA_loss: 1.3241e-12 - val_yB_loss: 7.9386e-09\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 110us/step - loss: 9.9791e-05 - yA_loss: 2.5129e-07 - yB_loss: 9.9540e-05 - val_loss: 2.5116e-05 - val_yA_loss: 6.5822e-07 - val_yB_loss: 2.4458e-05\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 2.3401e-05 - yA_loss: 1.1008e-07 - yB_loss: 2.3291e-05 - val_loss: 2.8022e-07 - val_yA_loss: 3.8342e-09 - val_yB_loss: 2.7639e-07\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 6.6087e-08 - yA_loss: 4.9513e-09 - yB_loss: 6.1136e-08 - val_loss: 1.3927e-08 - val_yA_loss: 3.6168e-09 - val_yB_loss: 1.0310e-08\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 116us/step - loss: 9.4212e-08 - yA_loss: 2.7206e-09 - yB_loss: 9.1492e-08 - val_loss: 6.9426e-08 - val_yA_loss: 2.3016e-09 - val_yB_loss: 6.7124e-08\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 113us/step - loss: 8.7331e-05 - yA_loss: 1.3407e-07 - yB_loss: 8.7197e-05 - val_loss: 1.4332e-05 - val_yA_loss: 9.1354e-08 - val_yB_loss: 1.4241e-05\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 112us/step - loss: 8.4023e-05 - yA_loss: 1.6901e-07 - yB_loss: 8.3854e-05 - val_loss: 1.6149e-05 - val_yA_loss: 1.5104e-08 - val_yB_loss: 1.6134e-05\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 9.9466e-07 - yA_loss: 4.4649e-09 - yB_loss: 9.9019e-07 - val_loss: 5.4070e-08 - val_yA_loss: 3.1569e-09 - val_yB_loss: 5.0913e-08\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 1.2420e-08 - yA_loss: 1.9768e-09 - yB_loss: 1.0444e-08 - val_loss: 2.0473e-09 - val_yA_loss: 1.2332e-09 - val_yB_loss: 8.1404e-10\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 120us/step - loss: 6.8605e-09 - yA_loss: 9.2048e-10 - yB_loss: 5.9400e-09 - val_loss: 4.8208e-09 - val_yA_loss: 5.2481e-10 - val_yB_loss: 4.2960e-09\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 1.1153e-09 - yA_loss: 4.2127e-10 - yB_loss: 6.9407e-10 - val_loss: 3.6345e-10 - val_yA_loss: 2.8406e-10 - val_yB_loss: 7.9393e-11\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 7.1087e-09 - yA_loss: 1.9218e-10 - yB_loss: 6.9165e-09 - val_loss: 4.6505e-09 - val_yA_loss: 5.0071e-11 - val_yB_loss: 4.6005e-09\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 4.2407e-06 - yA_loss: 6.2584e-09 - yB_loss: 4.2344e-06 - val_loss: 5.0578e-05 - val_yA_loss: 1.0392e-07 - val_yB_loss: 5.0474e-05\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 115us/step - loss: 5.9013e-05 - yA_loss: 1.2790e-07 - yB_loss: 5.8886e-05 - val_loss: 4.6867e-06 - val_yA_loss: 3.2426e-08 - val_yB_loss: 4.6543e-06\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 166us/step - loss: 1.0351e-04 - yA_loss: 1.5358e-07 - yB_loss: 1.0335e-04 - val_loss: 3.6304e-04 - val_yA_loss: 4.5346e-07 - val_yB_loss: 3.6259e-04\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 171us/step - loss: 5.5851e-05 - yA_loss: 1.1152e-07 - yB_loss: 5.5740e-05 - val_loss: 2.8655e-04 - val_yA_loss: 4.0905e-07 - val_yB_loss: 2.8614e-04\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 160us/step - loss: 8.9330e-05 - yA_loss: 1.4189e-07 - yB_loss: 8.9188e-05 - val_loss: 3.3869e-06 - val_yA_loss: 4.9233e-08 - val_yB_loss: 3.3377e-06\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 1.4538e-06 - yA_loss: 1.1421e-08 - yB_loss: 1.4423e-06 - val_loss: 1.4157e-06 - val_yA_loss: 2.4041e-09 - val_yB_loss: 1.4133e-06\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 163us/step - loss: 6.0273e-07 - yA_loss: 9.5379e-10 - yB_loss: 6.0177e-07 - val_loss: 9.0929e-09 - val_yA_loss: 6.3868e-11 - val_yB_loss: 9.0290e-09\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 161us/step - loss: 1.0843e-07 - yA_loss: 1.4342e-10 - yB_loss: 1.0829e-07 - val_loss: 1.7053e-09 - val_yA_loss: 5.9039e-12 - val_yB_loss: 1.6994e-09\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 8.1339e-09 - yA_loss: 1.0364e-11 - yB_loss: 8.1236e-09 - val_loss: 2.8422e-10 - val_yA_loss: 1.3384e-12 - val_yB_loss: 2.8288e-10\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 3.8397e-10 - yA_loss: 8.1760e-13 - yB_loss: 3.8316e-10 - val_loss: 6.7403e-12 - val_yA_loss: 9.6106e-14 - val_yB_loss: 6.6442e-12\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 7.1780e-10 - yA_loss: 1.5449e-12 - yB_loss: 7.1626e-10 - val_loss: 1.2120e-10 - val_yA_loss: 3.1258e-13 - val_yB_loss: 1.2089e-10\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 9.1135e-09 - yA_loss: 1.2511e-11 - yB_loss: 9.1010e-09 - val_loss: 2.0958e-07 - val_yA_loss: 2.5066e-10 - val_yB_loss: 2.0933e-07\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 4.8208e-07 - yA_loss: 7.4007e-10 - yB_loss: 4.8134e-07 - val_loss: 1.3652e-08 - val_yA_loss: 9.3997e-11 - val_yB_loss: 1.3558e-08\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 1.0775e-08 - yA_loss: 6.2956e-11 - yB_loss: 1.0712e-08 - val_loss: 3.0555e-11 - val_yA_loss: 6.7631e-12 - val_yB_loss: 2.3792e-11\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 117us/step - loss: 3.5800e-10 - yA_loss: 4.6849e-12 - yB_loss: 3.5331e-10 - val_loss: 2.6289e-11 - val_yA_loss: 1.7428e-12 - val_yB_loss: 2.4546e-11\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 8.4543e-09 - yA_loss: 1.5391e-11 - yB_loss: 8.4389e-09 - val_loss: 6.0594e-08 - val_yA_loss: 8.9137e-11 - val_yB_loss: 6.0505e-08\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 2.8106e-07 - yA_loss: 7.1952e-10 - yB_loss: 2.8034e-07 - val_loss: 5.2068e-08 - val_yA_loss: 1.6030e-10 - val_yB_loss: 5.1907e-08\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 1.6788e-04 - yA_loss: 2.3759e-07 - yB_loss: 1.6764e-04 - val_loss: 3.6747e-06 - val_yA_loss: 1.4394e-08 - val_yB_loss: 3.6603e-06\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 1.6903e-06 - yA_loss: 4.4291e-09 - yB_loss: 1.6858e-06 - val_loss: 8.9974e-09 - val_yA_loss: 6.9039e-10 - val_yB_loss: 8.3070e-09\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 161us/step - loss: 1.1740e-09 - yA_loss: 4.4697e-10 - yB_loss: 7.2704e-10 - val_loss: 6.6771e-10 - val_yA_loss: 3.1424e-10 - val_yB_loss: 3.5347e-10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yA_weights\n",
      " [array([[1.9999969]], dtype=float32), array([0.9999825], dtype=float32)] \n",
      "\n",
      "yB_weights\n",
      " [array([[2.0000434],\n",
      "       [3.0000079]], dtype=float32), array([0.99998426], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "a = 2; b = 1; c = 2; d = 3; e = 1\n",
    "\n",
    "x_train_A = np.random.rand(1000,1) * 2 - 1\n",
    "x_train_B = np.random.rand(1000,1) * 2 - 1\n",
    "y_train_A = a * x_train_A + b\n",
    "y_train_B = c * np.exp(y_train_A) + d * x_train_B + e\n",
    "\n",
    "xA = layers.Input((1,), name='xA')\n",
    "xB = layers.Input((1,), name='xB')\n",
    "yA = layers.Dense(1, name='yA')(xA)\n",
    "h1 = layers.Lambda(lambda x: K.exp(x), name='exp')(yA)\n",
    "h2 = layers.Concatenate(name='concat')([h1, xB])\n",
    "yB = layers.Dense(1, name='yB')(h2)\n",
    "\n",
    "model = models.Model([xA, xB], [yA, yB])\n",
    "model.summary()\n",
    "\n",
    "model.compile('adam', 'mse')\n",
    "hist = model.fit([x_train_A, x_train_B], [y_train_A, y_train_B],\n",
    "                 batch_size=8, epochs=200, validation_split=0.2)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "yA_weights = model.get_layer('yA').get_weights()\n",
    "yB_weights = model.get_layer('yB').get_weights()\n",
    "print('yA_weights\\n', yA_weights, '\\n')\n",
    "print('yB_weights\\n', yB_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
